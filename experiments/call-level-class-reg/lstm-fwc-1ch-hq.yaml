trainer:
  max_epochs: 30
  accelerator: "auto"
  move_metrics_to_cpu: True
  log_every_n_steps: 5
  fast_dev_run: False
  enable_checkpointing: True
  check_val_every_n_epoch: 1
  logger: 
    - class_path: whale.utils.loggers.CustomMLFLogger
      init_args:
        experiment_name: "call-level-class-reg"
        run_name: "lstm-fwc-1ch-hq"
        save_dir: "/network/projects/aia/whale_call/mlruns"

  callbacks:
  - class_path: pytorch_lightning.callbacks.EarlyStopping
    init_args:
      monitor: "overall_val_f1"
      patience: 5
      mode: "max"
      verbose: True
  - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    init_args:
      dirpath: "/network/projects/aia/whale_call/mlruns/ckpts/call-level-class-reg/lstm-fwc-1ch-hq"
      monitor: "overall_val_f1"
      mode: "max"
      auto_insert_metric_name: True
      save_on_train_epoch_end: True

data:
  class_path: whale.data_io.data_loader.WhaleDataModule
  init_args:
    data_dir: /network/projects/aia/whale_call/LABELS/fw_HQ_filt_mixed
    batch_size: 32
    data_type: spec

model:
  class_path: whale.models.LSTM
  init_args:
    input_dim: 129
    hidden_dim: 64

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-09
    weight_decay: 0.0002

lr_scheduler:
  class_path: torch.optim.lr_scheduler.ExponentialLR
  init_args:
    gamma: 0.1